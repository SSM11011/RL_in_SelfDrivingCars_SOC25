# -*- coding: utf-8 -*-
"""PPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FF6ZyyZzhZUfakNMhDqGDjb8fdXm9ZM6
"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import imageio

# Environment Setup
env = gym.make("FrozenLake-v1", is_slippery=False, map_name="4x4", render_mode="rgb_array")
n_states = env.observation_space.n
n_actions = env.action_space.n

# Hyperparameters
gamma = 0.99
epsilon = 0.1
lr_policy = 1e-3
lr_value = 1e-3
epochs = 5000
episodes_per_epoch = 100

# Initialize policy and value tables
policy = np.full((n_states, n_actions), 1.0 / n_actions)
value_table = np.zeros(n_states)

def softmax(x):
    x = x - np.max(x)
    e_x = np.exp(x)
    return e_x / np.sum(e_x)

def choose_action(state):
    probs = policy[state]
    return np.random.choice(n_actions, p=probs)

def compute_returns(rewards):
    G = 0
    returns = []
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    return returns

def update_policy(states, actions, old_probs, advantages):
    for i in range(len(states)):
        s = states[i]
        a = actions[i]
        prob = policy[s, a]
        ratio = prob / (old_probs[i] + 1e-8)
        clipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)
        policy[s] *= 1.0  # No-op for clarity; update happens below
        policy[s, a] += lr_policy * min(ratio * advantages[i], clipped_ratio * advantages[i])

        # Re-normalize
        policy[s] = softmax(policy[s])

def update_value_function(states, returns):
    for i in range(len(states)):
        s = states[i]
        v = value_table[s]
        value_table[s] += lr_value * (returns[i] - v)

# Training Loop
all_rewards = []

for epoch in range(epochs):
    batch_states = []
    batch_actions = []
    batch_rewards = []
    batch_old_probs = []
    batch_returns = []
    batch_advantages = []

    for episode in range(episodes_per_epoch):
        state, _ = env.reset()
        states = []
        actions = []
        rewards = []
        old_probs = []

        done = False
        while not done:
            action = choose_action(state)
            next_state, reward, done, _, _ = env.step(action)

            states.append(state)
            actions.append(action)
            rewards.append(reward)
            old_probs.append(policy[state, action])

            state = next_state

        returns = compute_returns(rewards)
        advantages = [returns[i] - value_table[states[i]] for i in range(len(states))]

        batch_states += states
        batch_actions += actions
        batch_rewards.append(sum(rewards))
        batch_old_probs += old_probs
        batch_returns += returns
        batch_advantages += advantages

    update_policy(batch_states, batch_actions, batch_old_probs, batch_advantages)
    update_value_function(batch_states, batch_returns)
    avg_reward = np.mean(batch_rewards)
    all_rewards.append(avg_reward)
    if epoch % 10000 == 0:
       print(f"Epoch: {epoch}, Average Reward: {avg_reward}")

# Plotting learning curve
plt.plot(all_rewards)
plt.title("PPO on FrozenLake-v1")
plt.xlabel("Epoch")
plt.ylabel("Average Reward")
plt.grid()
plt.show()

# Greedy action selection for rendering
def greedy_action(state):
    return np.argmax(policy[state])

# Record the agent's behavior into a GIF
with imageio.get_writer("frozenlake_ppo.gif", mode='I', duration=1, loop=0) as writer:
    state, _ = env.reset()
    done = False
    steps = 0
    while not done and steps < 200:
        frame = env.render()
        writer.append_data(frame)
        action = greedy_action(state)
        state, _, done, _, _ = env.step(action)
        steps += 1

