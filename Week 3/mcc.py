# -*- coding: utf-8 -*-
"""SoC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12uwUvWUIrdyCN3aLxqGSc6daiFkE94eD
"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import imageio

env = gym.make("FrozenLake-v1", is_slippery=False, map_name="4x4", render_mode="rgb_array")
env.reset()
plt.imshow(env.render())

n_states = env.observation_space.n
n_actions = env.action_space.n
Q = np.zeros((n_states, n_actions))
returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]
epsilon = 0.1
episodes = 10000
epsilon_decay = 0.9995
min_epsilon = 0.01
gamma = 0.99
rewards = []

# Training with Monte Carlo Control (First-Visit)
for ep in range(episodes):
    state = env.reset()[0]
    episode = []
    done = False
    total_reward = 0

    while not done:
        if np.random.rand() < epsilon:
          action = np.random.choice(n_actions)
        else:
          action = np.argmax(Q[state])
        next_state, reward, done, _, _ = env.step(action)
        episode.append((state, action, reward))
        state = next_state
        total_reward += reward

    rewards.append(total_reward)
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    G = 0
    visited = set()
    for t in reversed(range(len(episode))):
        s, a, r = episode[t]
        G = gamma * G + r
        if (s, a) not in visited:
            visited.add((s, a))
            returns[s][a].append(G)
            Q[s][a] = np.mean(returns[s][a])

# Plot rewards
plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))
plt.xlabel("Episode")
plt.ylabel("Average Reward (100ep)")
plt.title("Monte Carlo Control (First-Visit) - FrozenLake")
plt.grid()
plt.show()

with imageio.get_writer("frozenlake_mcc.gif", mode='I', duration=1, loop=0) as writer:
    state = env.reset()[0]
    done = False
    steps = 0
    while not done and steps < 200:
        frame = env.render()
        writer.append_data(frame)
        action = np.argmax(Q[state])
        state, _, done, _, _ = env.step(action)
        steps += 1