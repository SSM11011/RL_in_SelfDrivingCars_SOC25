# -*- coding: utf-8 -*-
"""SAC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zhke7uKdMm7aEE-f4r3dUqaRiP07TG-V
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt

# --- Hyperparameters ---
gamma = 0.99
alpha = 0.1  # entropy regularization coefficient
lr = 0.001
batch_size = 64
buffer_limit = 5000
tau = 0.005
episodes = 1000

# --- Environment Setup ---
env = gym.make("FrozenLake-v1", is_slippery=False, map_name="4x4")
n_states = env.observation_space.n
n_actions = env.action_space.n

# --- One-hot encoder for states ---
def one_hot(s):
    vec = np.zeros(n_states)
    vec[s] = 1.0
    return vec

# --- Replay Buffer ---
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def put(self, transition):
        self.buffer.append(transition)

    def sample(self, n):
        mini_batch = random.sample(self.buffer, n)
        states, actions, rewards, next_states, dones = zip(*mini_batch)
        return torch.FloatTensor(states), torch.LongTensor(actions), \
               torch.FloatTensor(rewards), torch.FloatTensor(next_states), \
               torch.FloatTensor(dones)

    def size(self):
        return len(self.buffer)

# --- Neural Networks ---
class QNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(n_states, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions)
        )

    def forward(self, x):
        return self.fc(x)

class PolicyNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(n_states, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions)
        )

    def forward(self, x):
        logits = self.fc(x)
        probs = torch.softmax(logits, dim=-1)
        log_probs = torch.log(probs + 1e-8)
        return probs, log_probs

# --- Training Setup ---
q1 = QNetwork()
q2 = QNetwork()
q1_target = QNetwork()
q2_target = QNetwork()
q1_target.load_state_dict(q1.state_dict())
q2_target.load_state_dict(q2.state_dict())

policy = PolicyNetwork()

q1_optimizer = optim.Adam(q1.parameters(), lr=lr)
q2_optimizer = optim.Adam(q2.parameters(), lr=lr)
policy_optimizer = optim.Adam(policy.parameters(), lr=lr)

buffer = ReplayBuffer(buffer_limit)
returns = []

# --- SAC Training Loop ---
for ep in range(episodes):
    s, _ = env.reset()
    done = False
    ep_reward = 0

    while not done:
        s_onehot = one_hot(s)
        s_tensor = torch.FloatTensor(s_onehot).unsqueeze(0)
        with torch.no_grad():
            action_probs, _ = policy(s_tensor)
        a = torch.distributions.Categorical(action_probs).sample().item()

        s_next, r, done, _, _ = env.step(a)
        buffer.put((s_onehot, a, r, one_hot(s_next), done))
        s = s_next
        ep_reward += r

        # --- Learn ---
        if buffer.size() > batch_size:
            states, actions, rewards, next_states, dones = buffer.sample(batch_size)

            with torch.no_grad():
                next_probs, next_log_probs = policy(next_states)
                q1_next = q1_target(next_states)
                q2_next = q2_target(next_states)
                min_q_next = torch.min(q1_next, q2_next)
                soft_q = next_probs * (min_q_next - alpha * next_log_probs)
                next_q_value = soft_q.sum(dim=1)

                target_q = rewards + gamma * (1 - dones) * next_q_value

            # Q1 Loss
            q1_pred = q1(states).gather(1, actions.unsqueeze(1)).squeeze()
            q1_loss = nn.functional.mse_loss(q1_pred, target_q)

            # Q2 Loss
            q2_pred = q2(states).gather(1, actions.unsqueeze(1)).squeeze()
            q2_loss = nn.functional.mse_loss(q2_pred, target_q)

            q1_optimizer.zero_grad()
            q1_loss.backward()
            q1_optimizer.step()

            q2_optimizer.zero_grad()
            q2_loss.backward()
            q2_optimizer.step()

            # Policy Update
            probs, log_probs = policy(states)
            q1_val = q1(states)
            q2_val = q2(states)
            min_q = torch.min(q1_val, q2_val)
            policy_loss = (probs * (alpha * log_probs - min_q)).sum(dim=1).mean()

            policy_optimizer.zero_grad()
            policy_loss.backward()
            policy_optimizer.step()

            # Soft update targets
            for param, target_param in zip(q1.parameters(), q1_target.parameters()):
                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
            for param, target_param in zip(q2.parameters(), q2_target.parameters()):
                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

    returns.append(ep_reward)
    if (ep + 1) % 50 == 0:
        print(f"Episode {ep+1}: Reward {ep_reward}")

# --- Plotting ---
plt.plot(returns)
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.title("Discrete SAC on FrozenLake")
plt.grid()
plt.show()

# Helper to choose greedy action using the learned policy
def greedy_action(state):
    state_onehot = one_hot(state)
    state_tensor = torch.FloatTensor(state_onehot).unsqueeze(0)
    with torch.no_grad():
        action_probs, _ = policy(state_tensor)
    return torch.argmax(action_probs).item()

# Record the agent's behavior into a GIF
env = gym.make("FrozenLake-v1", is_slippery=False, map_name="4x4", render_mode="rgb_array")
with imageio.get_writer("frozenlake_sac.gif", mode='I', duration=1, loop=0) as writer:
    state, _ = env.reset()
    done = False
    steps = 0
    while not done and steps < 200:
        frame = env.render()
        writer.append_data(frame)
        action = greedy_action(state)
        state, _, done, _, _ = env.step(action)
        steps += 1